{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b44f6af6",
   "metadata": {},
   "source": [
    "# Ensemble 1 Run de novo model with 5 different seeds with evaluation on balanced test set with all biol. classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda358b8",
   "metadata": {},
   "source": [
    "<b> What is our goal? <b>\n",
    "\n",
    "The main goal is to develop a model that has meaningful uncertainty, meaning the uncertainty is low for known and biologically uninteresting classes, but high for unknown and ideally biologically interesting classes. Our hope would be that this new model learns something the original classifier (autophagy_2_1 from SPACRSpy) did not, thereby identifying something new.\n",
    "The way to test this for the multi-class classifier is by leaving out different biological conditions during training and then checking uncertainty on them as well as by evaluate the new model with screening data, plotting its 8th layer in UMAP and investigating the classifcation scores between the old and new models. \n",
    "\n",
    "<b> What have we done so far? <b>\n",
    "\n",
    "So far we got for Case 1 the 8th layer activations plotted in a UMAP and identified screening hits. Prior to excising these cells, we need to ensure we are certain about which cells to excise and therefore we want to try different ensemble approaches and get a list of cell ids and their respective slide numbers which seem to overlap across the emsembling process and these cells we want to excise. This way we try to correct for the fact that our screening hits may contain technical artifacts. In other words, if we were to excise these 72 screening hits which are potentially contaminated with technical artifacts, we would see no enrichment of any gene. Thus we want to be computationally as confident as possible that these are actually interesting screening hits, then if there is nothing enriched, we at least learn something about the computational approach we used which means our model identifies some technical artifact very confidently. Within a single slide we expect batch effect. We found 72 screening hits in a subset of 7200 cells and a single slide has approx. 300000 cells and thus we expect approx. 3000 screening hits in total.\n",
    "\n",
    "<b> What data do we have now? <b>\n",
    "\n",
    "1. Stimulated 14h (or 16h) -> labelled as 0\n",
    "2. Unstimulated -> labelled as 1\n",
    "3. ATG5 KO (stimulated but that doesn’t matter, this KO supersedes the stim status [probably looks like unstimulated data]) -> labelled as 2\n",
    "4. Stimulated timecourse data -> labelled as 3\n",
    "5. EI24 KO timecourse data (more similar to unstim) -> labelled as 4\n",
    "6. Screening data (similar to stim) -> labelled as 5\n",
    "\n",
    "\n",
    "<b> What are we doing in this section? <b>\n",
    "\n",
    "Here we run the de novo model multiple times with 5 different random seeds. For each run, we identify the screening hits in UMAP, identify overlapping screening hits using intersection methods and thus hopefully identify consistent patterns and reduce the impact of random fluctuations.\n",
    "We do this all for Case 1(0,2 for training, and all other classes in test)\n",
    "\n",
    "<b> Why do we do this? <b>\n",
    "\n",
    "We run the ensembling to become more certain about if the screening hits we see are worth excising or not. Specifically, it allows us to explore the variability in classification and uncertainty across different splits of the data. This process can help identify patterns that a single de novo model run might have missed, especially in the context of uncertainty. We evaluate on how well the de novo model does on the EI24 knockout.\n",
    "Since El24 looks more distinctly different from both and thus our best positive control; if we can detect this based on uncertainty then we can detect novel biology!\n",
    "We want the UMAP on the test set to show all biological classes and also we want the images visualized of the screening hits in case there is something visually interesting.\n",
    "Also we need to cross reference with the cells we have already excised from autophagy_2_1 model as hits since that binary model can recognize anything which is unstim as a hit in the screen and thus we have to filter our resulting cells for those which hasn’t already been excised. \n",
    "\n",
    "<b> What to do from here? <b>\n",
    "\n",
    "1. We could also try the LOF as a score. By applying LOF we can gain an additional layer of confidence in 1. We could also try the LOF as a score. By applying LOF we can gain an additional layer of confidence in identifying truly novel phenotypes. LOF helps differentiate between points that are genuine anomalies and those that are within expected variability. \n",
    "2. After training the five models, we could ensemble them by averaging their predictions or using a majority vote for classification. This will help smooth out individual model variability and may highlight outliers or new patterns.\n",
    "3. We want to run Case 8 (0,2,4|test everything) as a positive control.\n",
    "4. We could run uncertainty on 3 or 4 classes (our other cases) and have a statistic on how well we do as we increase number of classes and ideally find a nice list of cell ids to excise which contain a novel phenotype.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d69c51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting torch-intermediate-layer-getter\n",
      "  Downloading torch_intermediate_layer_getter-0.1.post1.tar.gz (3.0 kB)\n",
      "Building wheels for collected packages: torch-intermediate-layer-getter\n",
      "  Building wheel for torch-intermediate-layer-getter (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for torch-intermediate-layer-getter: filename=torch_intermediate_layer_getter-0.1.post1-py3-none-any.whl size=3724 sha256=577f35209ddeaae2f1c62cf61ba0c5e974e0b0613fb6d10ec6f48147d81439da\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-rdw168y5/wheels/6a/11/c0/30d81aa26172d10d68ffaf352b0762eb9fe0a5f5dcf3de63e0\n",
      "Successfully built torch-intermediate-layer-getter\n",
      "Installing collected packages: torch-intermediate-layer-getter\n",
      "Successfully installed torch-intermediate-layer-getter-0.1.post1\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting umap-learn\n",
      "  Downloading umap_learn-0.5.6-py3-none-any.whl (85 kB)\n",
      "\u001b[K     |████████████████████████████████| 85 kB 3.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (0.56.4+1.g5f1bc7084)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from umap-learn) (4.65.0)\n",
      "Collecting pynndescent>=0.5\n",
      "  Downloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
      "\u001b[K     |████████████████████████████████| 56 kB 9.0 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.2.0)\n",
      "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.10.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.22.2)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn) (0.39.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from pynndescent>=0.5->umap-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->umap-learn) (3.1.0)\n",
      "Installing collected packages: pynndescent, umap-learn\n",
      "Successfully installed pynndescent-0.5.13 umap-learn-0.5.6\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting leidenalg\n",
      "  Downloading leidenalg-0.10.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.0 MB 3.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting igraph<0.12,>=0.10.0\n",
      "  Downloading igraph-0.11.6-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1 MB 30.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting texttable>=1.6.2\n",
      "  Downloading texttable-1.7.0-py2.py3-none-any.whl (10 kB)\n",
      "Installing collected packages: texttable, igraph, leidenalg\n",
      "Successfully installed igraph-0.11.6 leidenalg-0.10.2 texttable-1.7.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting scanpy==1.9.6\n",
      "  Downloading scanpy-1.9.6-py3-none-any.whl (2.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.0 MB 3.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.4 in /usr/local/lib/python3.10/dist-packages (from scanpy==1.9.6) (1.10.1)\n",
      "Collecting statsmodels>=0.10.0rc2\n",
      "  Downloading statsmodels-0.14.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.8 MB 24.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=0.24 in /usr/local/lib/python3.10/dist-packages (from scanpy==1.9.6) (1.2.0)\n",
      "Collecting anndata>=0.7.4\n",
      "  Downloading anndata-0.10.9-py3-none-any.whl (128 kB)\n",
      "\u001b[K     |████████████████████████████████| 128 kB 50.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: h5py>=3 in /usr/local/lib/python3.10/dist-packages (from scanpy==1.9.6) (3.9.0)\n",
      "Collecting patsy\n",
      "  Downloading patsy-0.5.6-py2.py3-none-any.whl (233 kB)\n",
      "\u001b[K     |████████████████████████████████| 233 kB 111.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: seaborn!=0.13.0 in /usr/local/lib/python3.10/dist-packages (from scanpy==1.9.6) (0.12.2)\n",
      "Requirement already satisfied: matplotlib>=3.4 in /usr/local/lib/python3.10/dist-packages (from scanpy==1.9.6) (3.7.1)\n",
      "Requirement already satisfied: networkx>=2.3 in /usr/local/lib/python3.10/dist-packages (from scanpy==1.9.6) (3.1)\n",
      "Requirement already satisfied: pandas!=2.1.2,>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scanpy==1.9.6) (1.5.2)\n",
      "Requirement already satisfied: numba>=0.41.0 in /usr/local/lib/python3.10/dist-packages (from scanpy==1.9.6) (0.56.4+1.g5f1bc7084)\n",
      "Requirement already satisfied: natsort in /usr/local/lib/python3.10/dist-packages (from scanpy==1.9.6) (8.4.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from scanpy==1.9.6) (1.2.0)\n",
      "Requirement already satisfied: umap-learn>=0.3.10 in /usr/local/lib/python3.10/dist-packages (from scanpy==1.9.6) (0.5.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from scanpy==1.9.6) (4.65.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from scanpy==1.9.6) (23.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from scanpy==1.9.6) (1.22.2)\n",
      "Collecting session-info\n",
      "  Downloading session_info-1.0.0.tar.gz (24 kB)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anndata>=0.7.4->scanpy==1.9.6) (1.1.1)\n",
      "Collecting array-api-compat!=1.5,>1.4\n",
      "  Downloading array_api_compat-1.8-py3-none-any.whl (38 kB)\n",
      "Collecting numpy>=1.17.0\n",
      "  Downloading numpy-2.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 16.3 MB 110.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.4->scanpy==1.9.6) (0.11.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.4->scanpy==1.9.6) (1.0.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.4->scanpy==1.9.6) (9.2.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.4->scanpy==1.9.6) (4.39.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.4->scanpy==1.9.6) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.4->scanpy==1.9.6) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.4->scanpy==1.9.6) (1.4.4)\n",
      "  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 17.1 MB 120.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.41.0->scanpy==1.9.6) (0.39.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.2,>=1.1.1->scanpy==1.9.6) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.4->scanpy==1.9.6) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24->scanpy==1.9.6) (3.1.0)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.10/dist-packages (from umap-learn>=0.3.10->scanpy==1.9.6) (0.5.13)\n",
      "Collecting stdlib_list\n",
      "  Downloading stdlib_list-0.10.0-py3-none-any.whl (79 kB)\n",
      "\u001b[K     |████████████████████████████████| 79 kB 40.7 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: session-info\n",
      "  Building wheel for session-info (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for session-info: filename=session_info-1.0.0-py3-none-any.whl size=8042 sha256=917762b68cd65c5bf5f5e32e61de8f8738f701644d5854ee9158d938d7933089\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-9kznlkq9/wheels/6a/aa/b9/eb5d4031476ec10802795b97ccf937b9bd998d68a9b268765a\n",
      "Successfully built session-info\n",
      "Installing collected packages: numpy, stdlib-list, patsy, array-api-compat, statsmodels, session-info, anndata, scanpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.22.2\n",
      "\u001b[33m    WARNING: Value for bin_prefix does not match. Please report this to <https://github.com/pypa/pip/issues/10151>\n",
      "    distutils: /usr/bin\n",
      "    sysconfig: /usr/local/bin\u001b[0m\n",
      "\u001b[33m    WARNING: Additional context:\n",
      "    user = False\n",
      "    home = None\n",
      "    root = None\n",
      "    prefix = None\u001b[0m\n",
      "    Uninstalling numpy-1.22.2:\n",
      "      Successfully uninstalled numpy-1.22.2\n",
      "Successfully installed anndata-0.10.9 array-api-compat-1.8 numpy-1.23.5 patsy-0.5.6 scanpy-1.9.6 session-info-1.0.0 statsmodels-0.14.3 stdlib-list-0.10.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: anndata in /usr/local/lib/python3.10/dist-packages (0.10.9)\n",
      "Requirement already satisfied: umap-learn in /usr/local/lib/python3.10/dist-packages (0.5.6)\n",
      "Requirement already satisfied: array-api-compat!=1.5,>1.4 in /usr/local/lib/python3.10/dist-packages (from anndata) (1.8)\n",
      "Requirement already satisfied: natsort in /usr/local/lib/python3.10/dist-packages (from anndata) (8.4.0)\n",
      "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.10/dist-packages (from anndata) (1.23.5)\n",
      "Requirement already satisfied: scipy>1.8 in /usr/local/lib/python3.10/dist-packages (from anndata) (1.10.1)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anndata) (1.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from anndata) (23.1)\n",
      "Requirement already satisfied: pandas!=2.1.0rc0,!=2.1.2,>=1.4 in /usr/local/lib/python3.10/dist-packages (from anndata) (1.5.2)\n",
      "Requirement already satisfied: h5py>=3.1 in /usr/local/lib/python3.10/dist-packages (from anndata) (3.9.0)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.2.0)\n",
      "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (0.56.4+1.g5f1bc7084)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (0.5.13)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from umap-learn) (4.65.0)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn) (0.39.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0rc0,!=2.1.2,>=1.4->anndata) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0rc0,!=2.1.2,>=1.4->anndata) (2023.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from pynndescent>=0.5->umap-learn) (1.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas!=2.1.0rc0,!=2.1.2,>=1.4->anndata) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->umap-learn) (3.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting watermark\n",
      "  Downloading watermark-2.5.0-py2.py3-none-any.whl (7.7 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from watermark) (65.5.1)\n",
      "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.10/dist-packages (from watermark) (6.6.0)\n",
      "Requirement already satisfied: ipython>=6.0 in /usr/local/lib/python3.10/dist-packages (from watermark) (8.14.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=1.4->watermark) (3.15.0)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=6.0->watermark) (0.1.6)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.0->watermark) (0.18.2)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.0->watermark) (3.0.38)\n",
      "Requirement already satisfied: traitlets>=5 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.0->watermark) (5.9.0)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=6.0->watermark) (0.7.5)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=6.0->watermark) (5.1.1)\n",
      "Requirement already satisfied: stack-data in /usr/local/lib/python3.10/dist-packages (from ipython>=6.0->watermark) (0.6.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.0->watermark) (4.8.0)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=6.0->watermark) (0.2.0)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.0->watermark) (2.15.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=6.0->watermark) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=6.0->watermark) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.0->watermark) (0.2.6)\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython>=6.0->watermark) (0.2.2)\n",
      "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython>=6.0->watermark) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython>=6.0->watermark) (2.2.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from asttokens>=2.1.0->stack-data->ipython>=6.0->watermark) (1.16.0)\n",
      "Installing collected packages: watermark\n",
      "Successfully installed watermark-2.5.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch-intermediate-layer-getter\n",
    "!pip install umap-learn\n",
    "!pip install leidenalg\n",
    "!pip install scanpy==1.9.6\n",
    "!pip install anndata umap-learn\n",
    "!pip install watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "093643bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting nexusformat\n",
      "  Downloading nexusformat-1.0.6-py3-none-any.whl (79 kB)\n",
      "\u001b[K     |████████████████████████████████| 79 kB 1.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from nexusformat) (1.10.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from nexusformat) (1.23.5)\n",
      "Requirement already satisfied: h5py>=2.9 in /usr/local/lib/python3.10/dist-packages (from nexusformat) (3.9.0)\n",
      "Collecting hdf5plugin\n",
      "  Downloading hdf5plugin-5.0.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (45.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 45.6 MB 8.5 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: hdf5plugin, nexusformat\n",
      "Successfully installed hdf5plugin-5.0.0 nexusformat-1.0.6\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nexusformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01b7151a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE! Installing ujson may make loading annotations faster.\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "\n",
    "# Import \n",
    "import os\n",
    "import wandb\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split, SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, balanced_accuracy_score\n",
    "import sys\n",
    "import seaborn as sn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch_intermediate_layer_getter import IntermediateLayerGetter as MidGetter\n",
    "import umap\n",
    "import scanpy as sc\n",
    "import anndata as ad\n",
    "import re\n",
    "from collections import Counter\n",
    "import random\n",
    "from torch.utils.data import Subset\n",
    "import h5py\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from sparcscore.ml.datasets import HDF5SingleCellDataset\n",
    "# from sparcscore.pipeline.project import TimecourseProject, Project\n",
    "# from sparcscore.pipeline.workflows import MultithreadedWGATimecourseSegmentation, WGATimecourseSegmentation, MultithreadedCytosolCellposeTimecourseSegmentation, ShardedWGASegmentation, ShardedDAPISegmentationCellpose, WGASegmentation, DAPISegmentationCellpose\n",
    "from sparcscore.pipeline.extraction import HDF5CellExtraction, TimecourseHDF5CellExtraction\n",
    "from sparcscore.pipeline.classification import MLClusterClassifier\n",
    "from sparcscore.ml.pretrained_models import autophagy_classifier2_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fcf3290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 3728222\n",
      "0: 500000\n",
      "1: 200000\n",
      "2: 300000\n",
      "3: 200000\n",
      "4: 135131\n",
      "5: 2393091\n"
     ]
    }
   ],
   "source": [
    "full_hdf5_data = HDF5SingleCellDataset(\n",
    "    dir_list=['/dss/dssfs02/lwp-dss-0001/pn36po/pn36po-dss-0001/di93kux/231018_EI24_timecourse_phenix/231018_0317_EI24_fixed_tc/single_cells.h5',\n",
    "              '/dss/dssfs02/lwp-dss-0001/pn36po/pn36po-dss-0001/di93kux/231018_EI24_timecourse_phenix/231018_0316_EI24_fixed_tc/single_cells.h5',\n",
    "              '/dss/dssfs02/lwp-dss-0001/pn36po/pn36po-dss-0001/di93kux/231018_EI24_timecourse_phenix/231018_0318_EI24_fixed_tc/single_cells.h5',\n",
    "              '/dss/dssfs02/lwp-dss-0001/pn36po/pn36po-dss-0001/di93kux/231004_autophagy_screen_6slides/2.3_A002/single_cells.h5', \n",
    "              '/dss/dssfs02/lwp-dss-0001/pn36po/pn36po-dss-0001/di93kux/231004_autophagy_screen_6slides/2.3_B004/single_cells.h5',\n",
    "              '/dss/dssfs02/lwp-dss-0001/pn36po/pn36po-dss-0001/di93kux/231004_autophagy_screen_6slides/2.3_D001/single_cells.h5',\n",
    "              '/dss/dssfs02/lwp-dss-0001/pn36po/pn36po-dss-0001/di93kux/231004_autophagy_screen_6slides/2.3_F003/single_cells.h5',\n",
    "              '/dss/dssfs02/lwp-dss-0001/pn36po/pn36po-dss-0001/di93kux/231004_autophagy_screen_6slides/2.3_H002/single_cells.h5',\n",
    "              '/dss/dssfs02/lwp-dss-0001/pn36po/pn36po-dss-0001/di93kux/231004_autophagy_screen_6slides/2.3_K001/single_cells.h5',\n",
    "              '/dss/dssfs02/lwp-dss-0001/pn36po/pn36po-dss-0001/di93kux/230714_autophagy_training_data_sample/T_01_stim_Cr203_C6_filtered.h5',\n",
    "              '/dss/dssfs02/lwp-dss-0001/pn36po/pn36po-dss-0001/di93kux/230714_autophagy_training_data_sample/T_02_stim_Cr203_C6_filtered.h5',\n",
    "              '/dss/dssfs02/lwp-dss-0001/pn36po/pn36po-dss-0001/di93kux/230714_autophagy_training_data_sample/T_01_stim_wt_filtered.h5',\n",
    "              '/dss/dssfs02/lwp-dss-0001/pn36po/pn36po-dss-0001/di93kux/230714_autophagy_training_data_sample/T_2.2_stim_wt_filtered.h5',\n",
    "              '/dss/dssfs02/lwp-dss-0001/pn36po/pn36po-dss-0001/di93kux/230714_autophagy_training_data_sample/T_2.3_stim_wt_filtered.h5',\n",
    "              '/dss/dssfs02/lwp-dss-0001/pn36po/pn36po-dss-0001/di93kux/230714_autophagy_training_data_sample/T_2.X_stim_wt_filtered.h5',\n",
    "              '/dss/dssfs02/lwp-dss-0001/pn36po/pn36po-dss-0001/di93kux/230714_autophagy_training_data_sample/T_02_stim_wt_filtered.h5',\n",
    "              '/dss/dssfs02/lwp-dss-0001/pn36po/pn36po-dss-0001/di93kux/230714_autophagy_training_data_sample/T_01_unstim_wt_filtered.h5',\n",
    "              '/dss/dssfs02/lwp-dss-0001/pn36po/pn36po-dss-0001/di93kux/230714_autophagy_training_data_sample/T_02_unstim_wt_filtered.h5',\n",
    "              '/dss/dssfs02/lwp-dss-0001/pn36po/pn36po-dss-0001/di93kux/230714_autophagy_training_data_sample/T_2.2_stim_Cr203_filtered.h5',\n",
    "              '/dss/dssfs02/lwp-dss-0001/pn36po/pn36po-dss-0001/di93kux/230714_autophagy_training_data_sample/T_2.3_stim_Cr203_filtered.h5',\n",
    "              '/dss/dssfs02/lwp-dss-0001/pn36po/pn36po-dss-0001/di93kux/230714_autophagy_training_data_sample/T_2.X_stim_Cr203_filtered.h5'],\n",
    "    dir_labels=[4, 4, 4, 5, 5, 5, 5, 5, 5, 3, 3, 0, 0, 0, 0, 0, 1, 1, 2, 2, 2], \n",
    "    root_dir='/dss/dssfs02/lwp-dss-0001/pn36po/pn36po-dss-0001/di93kux/230714_autophagy_training_data_sample/',\n",
    "    select_channel=4,  # Select the 5th channel (channel index 4)\n",
    "    return_id=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7c8ceccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to extract random cells from each class\n",
    "def sample_cells(dataset, class_label, n_samples, exclude_indices=[]):\n",
    "    indices = [i for i, cell in enumerate(dataset.data_locator) if cell[0] == class_label]\n",
    "    \n",
    "    # Remove any excluded indices\n",
    "    indices = list(set(indices) - set(exclude_indices))\n",
    "    \n",
    "    # Randomly sample n_samples indices from the available ones\n",
    "    return random.sample(indices, n_samples)\n",
    "\n",
    "# Create separate Test Set files for each class \n",
    "def create_testset(dataset, test_filename_template):\n",
    "    testset_indices = {}\n",
    "    \n",
    "    # For each class, sample 2000 cells and collect them\n",
    "    for class_label in range(6):\n",
    "        sampled_indices = sample_cells(dataset, class_label, 2000)\n",
    "        testset_indices[class_label] = sampled_indices\n",
    "        \n",
    "        # Save each class' test set to a separate HDF5 file\n",
    "        test_filename = test_filename_template.format(label=class_label)\n",
    "        \n",
    "        with h5py.File(test_filename, 'w') as f:\n",
    "            # Create a dataset for all the single cells in this class\n",
    "            cell_data_list = []\n",
    "            cell_index_list = []\n",
    "            \n",
    "            for idx in sampled_indices:\n",
    "                data, label = dataset[idx][0].numpy(), dataset[idx][1].item()\n",
    "                cell_data_list.append(data)\n",
    "                cell_index_list.append([idx, label])\n",
    "            \n",
    "            # Convert lists to numpy arrays\n",
    "            single_cell_data = np.array(cell_data_list)\n",
    "            single_cell_index = np.array(cell_index_list, dtype=np.uint64)\n",
    "            \n",
    "            # Create the datasets\n",
    "            f.create_dataset('single_cell_data', data=single_cell_data, dtype='float32')\n",
    "            f.create_dataset('single_cell_index', data=single_cell_index, dtype='uint64')\n",
    "    \n",
    "    return testset_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a7da6849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create separate Training Set files for selected classes\n",
    "def create_trainset(dataset, train_filename_template, testset_indices, class_labels, n_samples_per_class=100000):\n",
    "    trainset_indices = {}\n",
    "    testset_all_indices = set([idx for indices in testset_indices.values() for idx in indices])  # Flatten testset indices\n",
    "    \n",
    "    # Sample cells from the selected classes, excluding testset indices\n",
    "    for class_label in class_labels:\n",
    "        sampled_indices = sample_cells(dataset, class_label, n_samples_per_class, exclude_indices=testset_all_indices)\n",
    "        trainset_indices[class_label] = sampled_indices\n",
    "        \n",
    "        # Save each class' training set to a separate HDF5 file\n",
    "        train_filename = train_filename_template.format(label=class_label)\n",
    "        \n",
    "        with h5py.File(train_filename, 'w') as f:\n",
    "            # Create a dataset for all the single cells in this class\n",
    "            cell_data_list = []\n",
    "            cell_index_list = []\n",
    "            \n",
    "            for idx in sampled_indices:\n",
    "                data, label = dataset[idx][0].numpy(), dataset[idx][1].item()\n",
    "                cell_data_list.append(data)\n",
    "                cell_index_list.append([idx, label])\n",
    "            \n",
    "            # Convert lists to numpy arrays\n",
    "            single_cell_data = np.array(cell_data_list)\n",
    "            single_cell_index = np.array(cell_index_list, dtype=np.uint64)\n",
    "            \n",
    "            # Create the datasets\n",
    "            f.create_dataset('single_cell_data', data=single_cell_data, dtype='float32')\n",
    "            f.create_dataset('single_cell_index', data=single_cell_index, dtype='uint64')\n",
    "    \n",
    "    return trainset_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0ab192d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for overlap between testset and trainset indices\n",
    "def check_overlap(testset_indices, trainset_indices):\n",
    "    testset_all_indices = set([idx for indices in testset_indices.values() for idx in indices])  # Flatten testset indices\n",
    "    trainset_all_indices = set([idx for indices in trainset_indices.values() for idx in indices])  # Flatten trainset indices\n",
    "    overlap = testset_all_indices.intersection(trainset_all_indices)\n",
    "    \n",
    "    if overlap:\n",
    "        print(f\"Warning: Overlapping indices found between testset and trainset: {overlap}\")\n",
    "    else:\n",
    "        print(\"No overlap between testset and trainset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ef057f",
   "metadata": {},
   "source": [
    "##### Create balanced testset with instances from all biological classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3be5b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_filename_template = '/dss/dssfs02/lwp-dss-0001/pn36po/pn36po-dss-0001/di93quv/balanced_testset_all_classes/testset_{label}.h5'\n",
    "\n",
    "# Create test set with separate files for each class\n",
    "testset_indices = create_testset(full_hdf5_data, test_filename_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94be741",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Having a look if the test set has been created properly by looking into he h5 file\n",
    "#import nexusformat.nexus as nx\n",
    "#f = nx.nxload('./testset_0.h5')\n",
    "#print(f.tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c4af703f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 12000\n",
      "0: 2000\n",
      "1: 2000\n",
      "2: 2000\n",
      "3: 2000\n",
      "4: 2000\n",
      "5: 2000\n"
     ]
    }
   ],
   "source": [
    "# Now, create balanced dataset from the saved files\n",
    "balanced_testset_all_classes = HDF5SingleCellDataset(\n",
    "    dir_list=[f'/dss/dssfs02/lwp-dss-0001/pn36po/pn36po-dss-0001/di93quv/balanced_testset_all_classes/testset_{label}.h5' for label in range(6)],\n",
    "    dir_labels=[0, 1, 2, 3, 4, 5],  # class labels\n",
    "    root_dir='/dss/dssfs02/lwp-dss-0001/pn36po/pn36po-dss-0001/di93quv/balanced_testset_all_classes/',  \n",
    "    select_channel=4,  # Select the 5th channel\n",
    "    return_id=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe36af5",
   "metadata": {},
   "source": [
    "##### Case 1 balanced test set containing none of the balaneced testset instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e45979d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No overlap between testset and trainset.\n"
     ]
    }
   ],
   "source": [
    "# Define the class labels to include in the training set\n",
    "train_class_labels = [0, 2] \n",
    "\n",
    "# Create training set with separate files for selected class labels\n",
    "train_filename_template = '/dss/dssfs02/lwp-dss-0001/pn36po/pn36po-dss-0001/di93quv/training_sets/case1_balanced_trainingset/trainset_{label}.h5'\n",
    "trainset_indices = create_trainset(full_hdf5_data, train_filename_template, testset_indices, class_labels=train_class_labels)\n",
    "check_overlap(testset_indices, trainset_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3b9db05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 200000\n",
      "0: 100000\n",
      "2: 100000\n"
     ]
    }
   ],
   "source": [
    "balanced_trainset_class_0_and_2 = HDF5SingleCellDataset(\n",
    "    dir_list=[f'/dss/dssfs02/lwp-dss-0001/pn36po/pn36po-dss-0001/di93quv/training_sets/case1_balanced_trainingset/trainset_{label}.h5' for label in train_class_labels],\n",
    "    dir_labels=train_class_labels,  # The class labels\n",
    "    root_dir='/dss/dssfs02/lwp-dss-0001/pn36po/pn36po-dss-0001/di93quv/balanced_testset_all_classes/',  \n",
    "    select_channel=4,  # Select the 5th channel\n",
    "    return_id=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09170fa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c83946",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c943b9fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82516883",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc23724",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137d8513",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03402add",
   "metadata": {},
   "source": [
    "## II. Ensemble 1: De novo model with multiple random seeds <a class=\"anchor\" id=\"ensemble1\"></a>\n",
    "\n",
    "Here we run the de novo multi class classifier model multiple times with 5 different random seeds. For each run, we identify the screening hits in UMAP, identify overlapping screening hits using intersection methods. We do this to identify consistent patterns and reduce the impact of random fluctuations and technical artefacts and do it on the Monte Carlo dropout to get uncertainty estimates for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea616f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94db5ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Redirect print statements to a file\n",
    "sys.stdout = open(\"duplicate_multi_class_output.txt\", \"w\")\n",
    "\n",
    "# Initialize TensorBoard writer\n",
    "tensorboard_writer = SummaryWriter('runs/VGG2_autophagy_multi_class_training')\n",
    "\n",
    "# Log into W&B\n",
    "wandb.login()\n",
    "run = wandb.init(project=\"VGG2_autophagy_multi_class_training\")\n",
    "\n",
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create an instance of model\n",
    "num_classes = 3\n",
    "model = MultiClassClassifier(num_classes)\n",
    "model.to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 5  \n",
    "batch_size = 256 \n",
    "log_interval = 50  # Log metrics every 50 batches\n",
    "\n",
    "epsilon = 1e-8  # Small epsilon value to prevent log(0) in uncertainties\n",
    "\n",
    "# Set train and test data based on the scenario\n",
    "train_data = case1_hdf5_train_data\n",
    "test_data = case1_hdf5_test_data\n",
    "\n",
    "# Create DataLoaders\n",
    "train_data_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_data_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "stop_training = False\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    if stop_training:\n",
    "        break  \n",
    "    \n",
    "    print(\"Epoch: \", epoch)\n",
    "    model.train()  # Set model to training mode\n",
    "\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total_samples = len(train_data)\n",
    "    batch_counter = 0  # Reset batch counter at the start of each epoch\n",
    "\n",
    "    for batch_idx, (data, labels) in enumerate(train_data_loader):\n",
    "        data = data.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_function(output, labels)\n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.sum().item()\n",
    "        _, predicted = output.max(1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        batch_counter += 1\n",
    "        \n",
    "        # Calculate accuracy and average loss for the current batch\n",
    "        accuracy = 100.0 * correct / (batch_counter * batch_size)\n",
    "        average_loss = total_loss / (batch_counter * batch_size)\n",
    "\n",
    "        # Check for the desired accuracy and stop training if reached\n",
    "        if accuracy >= 99.0:\n",
    "            stop_training = True\n",
    "            print(\"Accuracy over 99% reached and thus stopping training...\")\n",
    "            break\n",
    "\n",
    "    # Calculate and log training metrics\n",
    "    all_train_labels = []\n",
    "    all_train_predicted = []\n",
    "\n",
    "    for data, labels in train_data_loader:\n",
    "        data = data.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        output = model(data)\n",
    "        _, predicted = output.max(1)\n",
    "        all_train_labels.extend(labels.cpu().numpy())\n",
    "        all_train_predicted.extend(predicted.cpu().numpy())\n",
    "\n",
    "    train_precision = precision_score(all_train_labels, all_train_predicted, average='macro')\n",
    "    train_recall = recall_score(all_train_labels, all_train_predicted, average='macro')\n",
    "    train_f1 = f1_score(all_train_labels, all_train_predicted, average='macro')\n",
    "    train_balanced_accuracy = balanced_accuracy_score(all_train_labels, all_train_predicted)\n",
    "\n",
    "    train_accuracy = accuracy_score(all_train_labels, all_train_predicted) * 100.0\n",
    "\n",
    "    print(\"Train Precision: \" + str(train_precision) + \" Recall: \" + str(train_recall) + \" F1 score: \" + str(train_f1))\n",
    "    print(\"Train Balanced Accuracy: {:.2f}%\".format(train_balanced_accuracy))\n",
    "    \n",
    "    # Log train metrics for the epoch\n",
    "    wandb.log({\n",
    "        \"Train Epoch\": epoch,\n",
    "        \"Train_Precision\": train_precision,\n",
    "        \"Train_Recall\": train_recall,\n",
    "        \"Train_F1-score\": train_f1,\n",
    "        \"Train_Balanced_Accuracy\": train_balanced_accuracy,\n",
    "        \"Train_Loss\": average_loss,\n",
    "    })\n",
    "    \n",
    "    # Log on TensorBoard\n",
    "    tensorboard_writer.add_scalar('Train_Precision', train_precision, global_step=epoch)\n",
    "    tensorboard_writer.add_scalar('Train_Recall', train_recall, global_step=epoch)\n",
    "    tensorboard_writer.add_scalar('Train_F1-score', train_f1, global_step=epoch)\n",
    "    tensorboard_writer.add_scalar('Train_Balanced_Accuracy', train_balanced_accuracy, global_step=epoch)\n",
    "    tensorboard_writer.add_scalar('Train_Loss', average_loss, global_step=epoch)\n",
    "\n",
    "    correct = 0\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # Test loop with dropout and aggregated confusion matrix\n",
    "    model.eval()\n",
    "    \n",
    "    # Enable dropout during testing\n",
    "    model.apply(lambda m: setattr(m, 'training', True))\n",
    "\n",
    "    test_correct = 0\n",
    "    test_average_loss = 0.0\n",
    "    all_test_labels = []\n",
    "    all_test_predicted = []\n",
    "    test_class_uncertainties = [[] for _ in range(num_classes)]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_data_loader:\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            output = model(data)\n",
    "            _, predicted = output.max(1)\n",
    "            test_correct += predicted.eq(labels).sum().item()\n",
    "            all_test_labels.extend(labels.cpu().numpy())\n",
    "            all_test_predicted.extend(predicted.cpu().numpy())\n",
    "\n",
    "            loss = loss_function(output, labels)\n",
    "            test_average_loss += loss.sum().item()\n",
    "\n",
    "            # Calculate class probabilities\n",
    "            probs = torch.nn.functional.softmax(output, dim=1)\n",
    "\n",
    "            # Calculate uncertainties (entropy) for each class\n",
    "            uncertainties = [-torch.sum(p * torch.log(p + epsilon)) for p in probs]\n",
    "\n",
    "            # Store uncertainties for each class\n",
    "            for i in range(num_classes):\n",
    "                class_uncertainty = np.mean(uncertainties[i].detach().cpu().numpy())\n",
    "                test_class_uncertainties[i].extend([class_uncertainty])\n",
    "\n",
    "    # Filter out predictions and labels for classes seen during training\n",
    "    mask_seen_classes = np.isin(all_test_labels, [0, 2])  # Class 0 and 2 seen at training\n",
    "    filtered_test_labels = np.array(all_test_labels)[mask_seen_classes]\n",
    "    filtered_test_predicted = np.array(all_test_predicted)[mask_seen_classes]\n",
    "\n",
    "    # Calculate accuracy and loss only for the seen classes\n",
    "    test_accuracy = accuracy_score(filtered_test_labels, filtered_test_predicted)\n",
    "    test_average_loss = test_average_loss / len(test_data)\n",
    "\n",
    "    print(\"Test Accuracy: {:.2f}%\".format(test_accuracy * 100))\n",
    "    print(\"Test Loss: {:.4f}\".format(test_average_loss))\n",
    "\n",
    "    wandb.log({\n",
    "        \"Test_Accuracy\": test_accuracy,\n",
    "        \"Test_Loss\": test_average_loss,\n",
    "    })\n",
    "\n",
    "    tensorboard_writer.add_scalar('Test_Accuracy', test_accuracy, global_step=epoch)\n",
    "    tensorboard_writer.add_scalar('Test_Loss', test_average_loss, global_step=epoch)\n",
    "    \n",
    "    \n",
    "    # Aggregate and log confusion matrix\n",
    "    aggregated_confusion = confusion_matrix(all_test_labels, all_test_predicted)\n",
    "\n",
    "    # Confusion matrix\n",
    "    epsilon = 1e-8\n",
    "    df_cm = pd.DataFrame(aggregated_confusion / (np.sum(aggregated_confusion, axis=1)[:, None] + epsilon),\n",
    "                         index=[i for i in range(num_classes)],\n",
    "                         columns=[i for i in range(num_classes)])\n",
    "\n",
    "    # Save confusion matrix to TensorBoard\n",
    "    figure = sn.heatmap(df_cm, annot=True).get_figure()\n",
    "    tensorboard_writer.add_figure(f'Aggregated Confusion Matrix - Epoch {epoch}', figure, global_step=epoch)\n",
    "    \n",
    "    # Set the model back to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Plot histogram of uncertainties\n",
    "    class1_uncertainties = test_class_uncertainties[1]\n",
    "    plt.hist(class1_uncertainties, bins=50, alpha=0.5, color='blue', label='Class 1 Uncertainties')\n",
    "    plt.xlabel('Uncertainty')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Uncertainty Distribution for Class 1')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'uncertainty_histogram_class1_epoch{epoch}.png')\n",
    "    plt.close()\n",
    "\n",
    "# Save model\n",
    "print(\"Saving final model now...\")\n",
    "torch.save(model.state_dict(), 'multi_class_VGG2_case1.pth')\n",
    "\n",
    "# Close the W&B run\n",
    "wandb.finish()\n",
    "\n",
    "# Close the TensorBoard writer\n",
    "tensorboard_writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
